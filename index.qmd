---
title: "Appendix"
---

## A1 Simulation settings and modeling

As simulation plays a crucial role in this study, we will explain the modeling process for the simulation agent. The simulator employed in this research is MuJoCo @todorov2012mujoco. 
The visualization of the simulation model is presented in @fig-sim-model.

The high-fidelity visual appearance of the model (@fig-sim-model A, C) utilized meshed files exported from CAD software. 
The simulated entity is modeled using boxes and capsules, as depicted in @fig-sim-model B, D. 
The mass and inertia properties were estimated by attributing appropriate materials to each component in the CAD software. 
Notably, the spine tendon is assigned a fixed length, enabling force transfer during pull behavior while exerting no force during push behavior. 
To more accurately replicate the mechanical characteristics of the flexible spine, each spine joint is modeled as a virtual spring-damper system. 
The parameters of these systems are determined via thorough evaluation using finite element analysis (FEA) simulations.
The tail is modeled similarly to the spine.
A revolute joint model is employed for the articulation of leg joints. 
Each motor within MuJoCo is assigned a control and torque range (see table @tbl-torque), consistent with the corresponding joint's action space and closely mimicking real-world servos' characteristics.

![Simulation model of the mouse robot in MuJoCo. (A) Side view of the high-fidelity visual appearance
of digital the mouse robot. 
(B) Side view of rigid body parts (transparent convex part), joints (blue arrows), and
tendons involved in the physical simulation.
(C) Top view of digital the mouse robot.
(D) Top view of simulation entity.](images/fig_sim_model_0.svg){#fig-sim-model fig-alt="Simulation model of the mouse robot in MuJoCo" fig-cap="Figure A5: Simulation model of the mouse robot in MuJoCo."}

| Motor        | Control range [rad] | Torque range [Nm] |
|--------------|---------------------|-------------------|
| Knee/elbow   | [-2.6, 1]         | [-0.157, 0.157] |
| Hip/shoulder | [-2.6, 0.2]         | [-0.157, 0.157] |
| Spine        | [-0.47, 0.47]       | [-0.157, 0.157] |

: Table A2: Control and torque ranges of motors in MuJoCo. {#tbl-torque tbl-colwidths="[30,35,35]"}

## A2 Policy training

We use the proximal policy optimization (PPO) algorithm @schulman2017proximal to train the neural network controller, MuJoCo @todorov2012mujoco as physics engine, OpenAI Gym @brockman2016openai as API between learning algorithm and environments in combination with Stable Baselines 3 @stable-baselines3, which provides open-source implementation of deep RL algorithms in Python.
The hyperparameters used for the PPO algorithm are given in table @tbl-hyperparameters.
<!-- The open-access code is provided at this repository\footnote{\url{https://github.com/zhenshan-bing/RL_Nermo}\label{refnote}}. -->
We train our policy network on a computer with an i7-8565U CPU. A total of 8 million time steps are used for training. The maximum number of time steps in an episode is 2000. With the environment settings of 10 ms per time step, the training takes about 22 hours in total simulation time and 3 hours in wall clock time for the policy to converge.

| Hyperparameter                                    | Value      |
|---------------------------------------------------|------------|
| Learning rate (`learning_rate`)                   | 0.0003   |
| Number of steps (`n_steps`)                       | 2048     |
| Minibatch size (`batch_size`)                     | 64       |
| Number of epochs (`n_epochs`)                     | 10       |
| Discount factor $\gamma$ (`gamma`)                | 0.99     |
| GAE lambda $\lambda$ (`gae_lambda`)               | 0.95     |
| Clip range (`clip_range`)                         | 0.2      |
| Entropy coefficient (`ent_coef`)                  | 0.0      |
| Value function coefficient (`vf_coef`)            | 0.5      |
: Table A3: Hyperparameters for PPO used in our experiments. {#tbl-hyperparameters tbl-colwidths="[50,50]"}


@fig-rewards A and B show the learning curves for the \textit{walking} environment.
In both settings, whether utilizing the spine or not, we observe a rapid increase in the overall reward as training progresses, alongside the primary reward that reflects the agent's velocity. 
The secondary reward, which reflects energy efficiency, starts at a low value at the onset of training, during which the agent is initially immobile. 
As the agent's movement capability develops, there is a corresponding rise in energy consumption. 
Towards the end of the training, when the weight of the secondary reward peaks, the agent demonstrates an improvement in moving efficiently, thereby reducing energy costs.
Initially, the penalty begins at a negative value, indicating the agent's unexpected contact with the ground. 
However, as training advances, this penalty steadily increases toward zero, signifying the development of a stable walking pattern.
In the end, all the reward components converge at a stable level.


@fig-rewards C and D illustrate the learning curves for the turning environment. 
The overall reward, which shows gradual improvement, indicates that the agent successfully learns to execute the turning task, adhering to a desired radius, and eventually reaches convergence at the end of the training stage. 
In the scenario involving the spine, the primary reward exhibits a more stable pattern and is significantly higher, suggesting that the spine plays a key role in facilitating smoother turning movements. 
In terms of the secondary reward, a pattern characterized by an initial increase, subsequent decrease, and eventual stabilization emerges. This phenomenon is intimately linked to the implementation of a termination criterion within the turning environments. This criterion dictates that an episode concludes when the agent's deviation from the turning center exceeds a threshold of $0.1$ meters relative to the commanded turning radius, or when the agent's torso makes contact with the ground. This measure is essential for ensuring that the agent's turning behavior remains aligned with the specified turning radius command.
The secondary reward value is closely associated with the duration of each episode. A pivotal factor in this relationship is the yaw angle of the agent's body, which serves as an indicator of its directional orientation. A significant divergence of the yaw angle from the intended direction (equal to the turned angle) results in a near-zero secondary reward, precipitating the termination of the episode due to directional inaccuracies.
At the beginning of the training process, there is an observable increase in both the secondary reward and the episode duration. This initial phase is marked by the agent's acquisition of skills to avoid contact with the ground, however, without achieving mobility. As the training progresses, a decline in these metrics is noted, attributable to the agent's increasing inclination to deviate from the designated path as it learns to move. 
This deviation results in shorter episode duration and, thus, lower secondary reward. 
In the final phase of training, a plateau in the episode duration and secondary reward is reached, indicating the agent's enhanced proficiency in executing turning maneuvers within the constraints of the desired turning radius. 
This trajectory of the secondary reward and episode duration is indicative of the learning process and the agent's ability to conform to turning radius commands.
Similar to the walking task, the penalty initially registers as a large negative value but rapidly stabilizes to zero, demonstrating the agent's capability for stable turning without unexpected behaviors.

![Learning curves of the walking and turning tasks.](images/fig_rewards_2.svg){#fig-rewards fig-alt="Learning curves of walking and turning tasks" fig-cap="Figure A6: Learning curves of the walking and turning tasks." }









## A3 Curriculum Weights Ablation study

<!-- ### A2.1 Curriculum Weights -->

We begin by highlighting the challenge of determining an appropriate energy consumption penalty weight.
This is done by examining the training curves at constant energy penalty weights of $0.04$, $0.07$, and $0.1$ ($\boldsymbol{k^t_c}$ in Eq.1).
Subsequently, we demonstrate the effectiveness of the reward curriculum approach by comparing its performance with training without the curriculum. 
The policies for this ablation study are trained in the \textit{walking} environment and with spinal flexion, as the additional spine actuation increases the training difficulty.

![Ablation study on the curriculum learning factor.](images/ablation_weight.svg){#fig-ablation-weight width="60%" fig-alt="Ablation study on curriculum learning" fig-cap="Figure A7: Ablation study on the curriculum learning factor $k_e$."}

@fig-ablation-weight illustrates that increasing the energy consumption penalty weight $k_e$ effectively reduces power consumption. 
However, this improvement comes at the expense of the primary goal, i.e., velocity tracking. 
For instance, applying a constant energy penalty of $0.1$ reduces the velocity tracking reward by approximately 30\% compared to an agent trained with a constant energy penalty of $0.04$. 
Moreover, using a constant energy penalty weight of 0.07 and 0.1 leads to substantial power and velocity tracking reward variances, indicating the instability of training, where the agent may become trapped in sub-optimal states like standing still or moving slowly to conserve energy.

In contrast to a fixed energy penalty weight, the reward curriculum approach gradually increases the energy penalty weight during training. 
By enabling the agent to focus on different criteria at various training stages, both training efficiency and final performance can be improved.  
As depicted in @fig-ablation-weight, the trial employing the reward curriculum (indicated by the red curve with $k^{max}_e = 0.1$) initially exhibits a rise in power consumption as it strives to achieve the primary objective (velocity tracking). 
On the one hand, over time, power consumption gradually diminishes as the weight parameter $k_e$ increases. 
On the other hand, the reward associated with the primary goal continues to increase steadily until full convergence is achieved.
When comparing the velocity-tracking reward curve between training with a reward curriculum and training without one, it becomes evident that the reward curriculum plays a vital role in stabilizing the training process. 
This is especially noticeable when a high energy penalty weight of $0.1$ is employed. With the reward curriculum, the results show a substantial increase in velocity-tracking rewards and reduced variance. 
This outperforms training performance with a constant energy penalty weight set at 0.07 and 0.1.
This method allows a higher energy penalty weight to decrease power consumption while preventing instability and failure in learning the velocity tracking caused by a high constant energy penalty weight. 
As shown in @fig-ablation-weight, the energy efficiency is effectively improved, surpassing the energy efficiency achieved through constant energy penalties of 0.04 and 0.07.

<!-- ### A2.2 Action clipping

In this section, we illustrate the impact of action space clipping on the acquisition of optimal locomotion skills.

![The ablation study on the action space clipping.](images/ablation_clipping.svg){#fig-ablation-clipping width="60%" fig-alt="Ablation study on action space clipping" fig-cap="Figure A8: The ablation study on the action space clipping."} -->

## A4 Central pattern generator





The proposed sim-to-real approach encodes the learned gait as a combination of rhythmic signals, each linked to a specific motor. 
However, this method has its limitations: it can only generate a set of distinct gait parameters tailored for anticipated behaviors and lacks the ability to modify its gait online. 
% In the ever-changing environment of real-world applications, this rigidity is a significant drawback. 
To address this issue, we suggest the integration of a central pattern generator (CPG) controller. 
This addition aims to implement the gait transition process, facilitating a seamless shift from one set of gait parameters to another, thus enhancing adaptability in practical tasks. 

![Central pattern generator controller for modifying gait parameters.The concept visualization of the CPG controller. 
Notably, the nine neurons in the CPG controller correspond to the nine motors of the robot. 
(B), (C), (D), and (E) illustrate the motor oscillations influenced by parameter transitions, wherein the parameters (defined in Eq.13) include frequency  $f$, amplitude $A$, mean offset $D$, and phase shift $\varphi$. 
The black points mark the initiation of changes in gait parameters. The red line represents the parameter change of the default controller, which directly switches gait parameters from one group to another.
The blue line represents the parameter change of the CPG controller.](images/cpg.svg){#fig-cpg fig-alt="Learning curves of walking and turning tasks" fig-cap="Figure A6: Learning curves of the walking and turning tasks." }


As illustrated in @fig-cpg A, the CPG controller is composed of nine neurons. 
Each neuron is responsible for the oscillation of one of the nine motors that generate the learned gaits. 
Notably, a central neuron is tasked with modulating spinal flexion and coordinating limb movement. 
For each limb, a pair of neurons control the motor oscillations, namely, one for the elbow/knee joint and another for the shoulder/hip joint. 
The oscillations of these two neurons are interdependent, ensuring coordinated movement.
% In this context, the oscillator of neurons can be defined as
The oscillator model of the neuron is defined as
$$
\begin{cases}
x(t) = A \cdot \cos(2 \pi f t + \varphi) + D, \\
s(t) = A \cdot \sin(2 \pi f t + \varphi) + D, \\
r'(t) = \sqrt{{x(t)}^2+{s(t)}^2}, \\
\frac{\partial x(t)}{\partial t}  =k_p(\frac{{A}^2-{r'(t)}^2}{A})\frac{x(t)}{A} - 2 \pi f s(t), \\
\frac{\partial s(t)}{\partial t} =k_p(\frac{{A}^2-{r'(t)}^2}{A})\frac{s(t)}{A} + 2 \pi f x(t).
\end{cases}
$$ {#eq-oscillator}
The motor command $s(t)$ is defined in @eq-oscillator. 
%Considering the oscillator is formulated on the theory of circular limit cycle, an additional hidden variable $x(t)$ is introduced to complement $y(t)$ and facilitate the formation of a circular limit cycle.
%The real-time radius of the virtual circular limit cycle formed by $x(t)$ and $y(t)$ is denoted as $r'(t)$. 
Considering the oscillator is formulated on the theory of circular limit cycle, an additional hidden variable $x(t)$ is introduced to complement $s(t)$ and facilitate the formation of a virtual circular limit cycle.
The radius of the virtual cycle formed by $x(t)$ and $s(t)$ is denoted as $r'(t)$.
And $k_p$ is a constant that affects the convergence rate of the circular limit cycle.
Based on the oscillator defined in @eq-oscillator, the changes of $s(t)$ and $x(t)$ over time step are
$$
\begin{cases}
s(t) = s(t-\sigma) + \sigma \cdot \frac{\partial s(t-\sigma)}{\partial t} \text{,}\\
x(t) = x(t-\sigma) + \sigma \cdot \frac{\partial x(t-\sigma)}{\partial t} \text{,}
\end{cases}
$$ 
where $\sigma$ is the minimal time step. 
By utilizing neurons modeled after this oscillator, the implemented CPG controller adeptly facilitates smooth transitions in gait parameters.
@fig-cpg B, C, D, and E provide a comparative analysis of the control parameter transitions between the default controller and the CPG controller.
In this context, the developed CPG controller can effectively ensure smooth gait transitions.

## A5 Model-based controller and optimization

<!-- In our prior work \cite{9981674}, we developed a trot gait that can leverage the lateral flexion of the spine, which outperforms non-spine-based trot gait in terms of velocity. -->
Three main factors determine the performance of a particular trot gait, namely, the gait frequency, lateral spine flexion, and normalized stride length.
The normalized stride length is defined as the percentage of the maximum stride length of the trot gait.
To ensure a fair comparison between the model-based controller and the neural network controller, we first utilize the grid search method to generate a variety of gaits and identify parameter combinations that yield the best energy efficiency for three gaits.
Since the chosen range and intervals of the gird search parameters can also influence the performance of gaits, we further adopt the Bayesian optimization method to select energy-efficient gaits for a fair comparison.

The grid search method generates a Cartesian product from the parameters in @tbl-grid-search-param, resulting in $1900$ sets of parameters for the trot gait. 
Each set of motion parameters is then tested by simulating 1000 steps in the simulation environment. 
To collect valid experimental data, we ignore the first $200$ time steps and evaluate the remaining 800 time steps. 
This is because we have observed that the robot requires approximately 200 time steps to
accelerate before it moves at a steady speed.

| Parameters                     | Values                                                                |
|:-------------------------------|:----------------------------------------------------------------------|
| Frequency [Hz] | [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6,1.8, 2.0]              |
| Spine angle [deg] | [0, 2, 4, 8, 10, 12, 14, 16, 18, 20]                               |
| Normalized stride length [%] | [10, 15, 20, 25, 30, 35, 40, 45, 50, 55,60, 65, 70, 75, 80, 85, 90, 95, 100] |

: Gait parameters used for the grid search algorithm for the trot walking. {#tbl-grid-search-param tbl-colwidths="[35, 65]"}

The goal of the Bayesian optimization method is to identify an optimized set of parameters that enable the robot to achieve the fastest speeds for three types of gaits. 
The parameter boundaries are set to the same values as those listed in Table @tbl-grid-search-param. 
<!-- Implementation can be found in the source code./ -->

![Performance of the model-based controller.
This scatter plot shows the energy-consumption
results of controllers at a range of velocities in the simulation.
The blue and orange dots represent the performance of the model-based controller optimized by the grid search algorithm, with and without using the spine. 
The red squares and red triangles represent the performance of the controller optimized by the Bayesian optimization, with and without using the spine.](images/bayesian_nospine_all.svg){#fig-baye width="60%"}

The power consumption and corresponding velocity results from the grid search algorithm are shown in @fig-baye as a point cloud of gaits using dot markers. 
The orange dots are the gaits without the spine and the blue dots are the gaits with the spine.
At varying velocities within the point cloud, the lowest points exhibit the highest levels of energy efficiency. Notably, we observe that as velocity increases, the power consumption of these gaits also rises. 
The red triangles and red squares represent the energy-efficient gaits that were discovered through Bayesian optimization. 
These gaits closely align with the top-performing gaits identified by the grid search algorithm. 
This further shows the effectiveness of Bayesian optimization in locating optimal gaits within a parameter space.
This Figure also clearly illustrates the advantage of utilizing the lateral spine for achieving energy-efficient locomotion.

## References {.unnumbered}